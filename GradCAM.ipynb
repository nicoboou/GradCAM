{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Third party imports\n",
    "import torch\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "# Local application imports\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(pretrained=True)\n",
    "resnet34.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
    "\n",
    "\n",
    "Input image must be of size (3x224x224). \n",
    "\n",
    "First convolution layer with maxpool. \n",
    "Then 4 ResNet blocks. \n",
    "\n",
    "Output of the last ResNet block is of size (512x7x7). \n",
    "\n",
    "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256), \n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"data\")\n",
    "os.mkdir(\"data/TP2_images\")\n",
    "!cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The images should be in a *sub*-folder of \"data/\" (ex: data/TP2_images/images.jpg) and *not* directly in \"data/\"!\n",
    "# otherwise the function won't find them\n",
    "dir_path = \"data/\" \n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the orignal image \n",
    "index = 12\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
    "- | - \n",
    "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grad_cam import VanillaGradCAM\n",
    "import cv2\n",
    "\n",
    "for i, val in enumerate(dataset):\n",
    "  \n",
    "    original_img = np.array(Image.open(dataset.imgs[i][0]).convert('RGB'))\n",
    "    resized_img = np.float32(cv2.resize(original_img, (224, 224))) / 255\n",
    "    input_tensor = dataset[i][0].view(1, 3, 224, 224)\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    resnet34.eval()\n",
    "\n",
    "    # Create an instance of GradCAM\n",
    "    gradcam = VanillaGradCAM(resnet34, target_layer=32)\n",
    "    \n",
    "    output = resnet34(input_tensor)\n",
    "    values, indices = torch.topk(output, 3)\n",
    "\n",
    "    # Subplots for \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "     \n",
    "    for i, val in enumerate(indices[0].numpy()):\n",
    "        target_class = classes[val]\n",
    "        # Generate a GradCAM heatmap\n",
    "        cam = gradcam.generate_class_activation_maps(input_tensor=input_tensor, classes_dict=classes,target_class=target_class) #Egyptian cat = 285 | tiger cat = 282\n",
    "        heatmap = gradcam.display_heatmap(img=resized_img,cam=cam,use_rgb=True,colormap = cv2.COLORMAP_JET, image_weight = 0.5)\n",
    "\n",
    "        ax[i].imshow(heatmap)\n",
    "        ax[i].set_title(target_class)\n",
    "        ax[i].axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "Looking at the results for each image in the dataset, it appears that almost time, the heatmap propagated on the top-1 class activation (plotting the gradient of the last ``nn.Conv2D`` layer) appears to be the most relevant. But in some cases, the top-3 classes are not so different, and the heatmap is not so clear, with even some better heatmaps for the other classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "- For many images analysed, all the heatmaps (i.e. performed for each of the top-3 classes) demonstrates that the same features/patterns are used for the model to predict the class involved.\n",
    "\n",
    "- To elaborate, for many images the predicted classes belong to the same \"category\" (e.g. felines with tiger cat, lion, puma). That is, they share many of the same characteristics, thus the same region will be highlighted in the heatmaps.\n",
    "\n",
    "- On the other hand, some heatmaps diverge a lot between the classes they try to explain. Taking the image n°5 (the laying dog), the heatmap for the class \"Norwegian elkhound\" is very different from the heatmap for the class \"Cardigan\". The first one highlights the dog's head, while the second one highlights the dog's tail. Thus displaying multiple heatmaps for the same image can be very useful to understand the model's reasoning and the small difference between the classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "To conclude Gradient-weighted Class Activation Maps (Grad-CAM) allows us to visualize the regions in an input image that the deep neural network focuses on for making its predictions, and genuilel helps us gain insights into the internal workings of the network, given insights on how it interprets the visual features of an image to make predictions.\n",
    "\n",
    "In our task, Grad-CAM was applied to classify different animal species and even different breeds within the same species. By using Grad-CAM, we were able to better understand the characteristics that the network pays attention to when making its predictions. For example, we may have discovered that the network relies on specific patterns in the fur or unique markings to differentiate between animal species and breeds. This information can be useful in improving the accuracy of further predictions, as well as in designing new and more efficient neural network architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centrale_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "773e5166209b1edcf178363ea559a77845b4d53698bee8a396c54c2d471ec9c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
